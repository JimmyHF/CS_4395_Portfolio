{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet Portfolio Assignment\n",
        "Jimmy Harvin\n",
        "\n"
      ],
      "metadata": {
        "id": "w2AbFD11PTIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet is a NLTK-integrated database developed at Princeton that contains semantic relationships and word definitions. Each word has a synset, or synonym set, containing all words relating to a given concept with definitions and examples. Words are organized in hierarchies, with is-a and in-a relationships making up the bulk of these hierarchies. Only content words appear in WordNet, and stop words are exempt from these hierarchies."
      ],
      "metadata": {
        "id": "s-W8TSr8P57H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duiIq3U7N4-C",
        "outputId": "62bac154-3559-4a55-b7d6-dda363ab2ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Text: Inaugural Address Corpus>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "import math\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.book import *\n",
        "text4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noun"
      ],
      "metadata": {
        "id": "7aIjZ54gIKVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets(\"value\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-ipor4PQ2S5",
        "outputId": "58e1c644-5a62-4f37-f33c-ed3fafa7d88c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('value.n.01'),\n",
              " Synset('value.n.02'),\n",
              " Synset('value.n.03'),\n",
              " Synset('value.n.04'),\n",
              " Synset('value.n.05'),\n",
              " Synset('value.n.06'),\n",
              " Synset('value.v.01'),\n",
              " Synset('prize.v.01'),\n",
              " Synset('respect.v.01'),\n",
              " Synset('measure.v.04'),\n",
              " Synset('rate.v.03')]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_synset = wn.synset(\"value.n.02\")\n",
        "print(val_synset.definition())\n",
        "print(val_synset.examples())\n",
        "print(val_synset.lemmas())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXT-sKjJULRs",
        "outputId": "deada51a-94d0-4c1d-c812-eeb6150e5ac7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quality (positive or negative) that renders something desirable or valuable\n",
            "['the Shakespearean Shylock is of dubious value in the modern world']\n",
            "[Lemma('value.n.02.value')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyper = val_synset.hypernyms()[0]\n",
        "top = wn.synset(\"entity.n.01\")\n",
        "\n",
        "while hyper:\n",
        "  print(hyper)\n",
        "  if hyper == top:\n",
        "    break\n",
        "  elif hyper.hypernyms():\n",
        "    hyper = hyper.hypernyms()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdcXb2U8WQMR",
        "outputId": "547c30fb-2885-410e-9d13-690c982e7374"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('worth.n.02')\n",
            "Synset('quality.n.01')\n",
            "Synset('attribute.n.02')\n",
            "Synset('abstraction.n.06')\n",
            "Synset('entity.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet is organized as a series of hypernyms and holonyms. Hypernyms and hyponyms form an is-a relationship, with a hypernym being a more general form of a given hyponym. All nouns can be traced to the top hypernym 'entity', but verbs and adverbs do not have this sort of tree structure. Holonyms and meronyms form an in-a relationship, with a holonym being a whole and meronyms being parts of that whole. Synsets in WordNet have lists of hypernyms/hyponyms and holonyms/meronyms, and these lists can be traversed to find more generic or specific synsets."
      ],
      "metadata": {
        "id": "RJ2HuBnBW6oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hypernyms: \", end = \"\")\n",
        "print(val_synset.hypernyms())\n",
        "print(\"Hyponyms: \", end = \"\")\n",
        "print(val_synset.hyponyms())\n",
        "print(\"Holonyms: \", end = \"\")\n",
        "print(val_synset.member_holonyms())\n",
        "print(\"Meronyms: \", end = \"\")\n",
        "print(val_synset.member_meronyms())\n",
        "print(\"Antonyms: \", end = \"\")\n",
        "print(val_synset.lemmas()[0].antonyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6pv5MseY4b5",
        "outputId": "d4df09d0-93c2-41a7-e91b-7104f7caa63c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms: [Synset('worth.n.02')]\n",
            "Hyponyms: [Synset('book_value.n.01'), Synset('gross_domestic_product.n.01'), Synset('gross_national_product.n.01'), Synset('importance.n.01'), Synset('invaluableness.n.01'), Synset('market_value.n.01'), Synset('monetary_value.n.01'), Synset('national_income.n.01'), Synset('par_value.n.01'), Synset('price.n.03'), Synset('richness.n.04'), Synset('standard.n.04'), Synset('unimportance.n.02')]\n",
            "Holonyms: []\n",
            "Meronyms: []\n",
            "Antonyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verb"
      ],
      "metadata": {
        "id": "i9m3-sW5IP2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_verb = wn.synset(\"value.v.01\")\n",
        "print(val_verb.definition())\n",
        "print(val_verb.examples())\n",
        "print(val_verb.lemmas())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocQEhutiII2d",
        "outputId": "05faad95-7e7e-42fd-afa3-7fc58bd50da1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fix or determine the value of; assign a value to\n",
            "['value the jewelry and art work in the estate']\n",
            "[Lemma('value.v.01.value')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_verb = val_verb.hypernyms()[0]\n",
        "\n",
        "while hyper_verb:\n",
        "  print(hyper_verb)\n",
        "  if len(hyper_verb.hypernyms()) > 0:\n",
        "    hyper_verb = hyper_verb.hypernyms()[0]\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2xSeUsfJ2nV",
        "outputId": "940646f6-795f-43f7-b1a0-5eb0c9178e5f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('determine.v.03')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verbs can also be part of a hypernym and hyponym relationship, though all verbs cannot be traced back to a single umbrella term. Rather than a meronym and holonym relationship, verbs can have troponyms, or more specific instances of an action, such as walking as opposed to moving."
      ],
      "metadata": {
        "id": "NY5KQNuwL3Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Morphy"
      ],
      "metadata": {
        "id": "5MJ2arSEMcN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.morphy(\"valuing\", wn.NOUN))\n",
        "print(wn.morphy(\"valuing\", wn.VERB))\n",
        "print(wn.morphy(\"valuing\", wn.ADJ))\n",
        "\n",
        "print(wn.morphy(\"valuer\", wn.NOUN))\n",
        "print(wn.morphy(\"valuer\", wn.VERB))\n",
        "print(wn.morphy(\"valuer\", wn.ADJ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY8npbr-MZiQ",
        "outputId": "9e6179ca-4590-4eb0-defd-9a153a067804"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "value\n",
            "None\n",
            "valuer\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity"
      ],
      "metadata": {
        "id": "QRPPHPpLPyRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.synsets(\"worth\"))\n",
        "worth = wn.synset(\"worth.n.01\")\n",
        "print(worth.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92B19pvyPz5t",
        "outputId": "8051c706-da63-4f1b-dc84-1415740eefc9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('worth.n.01'), Synset('worth.n.02'), Synset('worth.n.03'), Synset('deserving.s.01'), Synset('worth.s.02')]\n",
            "an indefinite quantity of something having a specified value\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Path similarity: \", end = \"\")\n",
        "print(val_synset.path_similarity(worth))\n",
        "print(\"Wu-Palmer similarity: \", end = \"\")\n",
        "print(wn.wup_similarity(val_synset, worth))\n",
        "\n",
        "context = [\"This\", \"gold\", \"has\", \"a\", \"high\", \"value\"]\n",
        "print(\"\\nLesk algorithm: \", end = \"\")\n",
        "print(lesk(context, 'worth'))\n",
        "print(wn.synset(\"worth.s.02\").definition())\n",
        "print(lesk(context, 'worth', pos = \"n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwjhTWYcQLbs",
        "outputId": "6dc0703a-06b1-4705-81ad-4e7848befb41"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path similarity: 0.125\n",
            "Wu-Palmer similarity: 0.36363636363636365\n",
            "\n",
            "Lesk algorithm: Synset('worth.s.02')\n",
            "having a specified value\n",
            "Synset('worth.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_synset.path_similarity(wn.synset(\"worth.n.02\")))\n",
        "print(wn.wup_similarity(val_synset, wn.synset(\"worth.n.02\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7EtvSy5VEmb",
        "outputId": "6134a400-6d0c-42d7-a3d4-3fa4fe04e4dc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I expected a higher numbers for path and Wu-Palmer similarity, given that these definitions are almost synonymous, but after double-checking, the version of 'worth' that appears as a hypernym of this instance of 'value' is 'worth.n.01', which has a more abstract definition. The similarities between 'value.n.02' and 'worth.n.02' are much higher. The Lesk algorithm is easy to understand, as it just tries to find overlap between word definitions and usage. I do not entirely get the point of this, though, as parts of definitions are rarely used in context, and overlap between usage and WordNet examples would likely be more valuable."
      ],
      "metadata": {
        "id": "5tL1d1GeToHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentiWordNet"
      ],
      "metadata": {
        "id": "TgLhsbYDV4Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.synsets(\"disgraceful\"))\n",
        "disgrace = wn.synset(\"disgraceful.s.01\")\n",
        "print(disgrace.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TKoUXSpV85m",
        "outputId": "3d395a89-5c36-4d01-cc3e-07cc26928095"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('disgraceful.s.01'), Synset('black.s.12')]\n",
            "giving offense to moral sensibilities and injurious to reputation; ; - Thackeray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for senti in swn.senti_synsets(\"disgraceful\"):\n",
        "  print(senti.synset)\n",
        "  print(\"Positive: \" + str(senti.pos_score()))\n",
        "  print(\"Negative: \" + str(senti.neg_score()))\n",
        "  print(\"Objective: \" + str(senti.obj_score()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AygMj2xRgHPX",
        "outputId": "0471f5ff-cf72-4e57-c221-7324be213750"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('disgraceful.s.01')\n",
            "Positive: 0.0\n",
            "Negative: 0.5\n",
            "Objective: 0.5\n",
            "Synset('black.s.12')\n",
            "Positive: 0.125\n",
            "Negative: 0.5\n",
            "Objective: 0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "senti_sentence = \"I am unbelievably stressed about these upcoming interviews\"\n",
        "senti_list = senti_sentence.split(\" \")\n",
        "\n",
        "print(senti_list)\n",
        "for word in senti_list:\n",
        "  if len(list(swn.senti_synsets(word))) > 0:\n",
        "    print(list(swn.senti_synsets(word))[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOaOHgF_ipZQ",
        "outputId": "e7963a2e-2a56-4736-b636-3af9e645aa1c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'unbelievably', 'stressed', 'about', 'these', 'upcoming', 'interviews']\n",
            "<iodine.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<americium.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<incredibly.r.01: PosScore=0.0 NegScore=0.5>\n",
            "<stress.v.01: PosScore=0.0 NegScore=0.0>\n",
            "<about.s.01: PosScore=0.0 NegScore=0.0>\n",
            "<approaching.s.01: PosScore=0.0 NegScore=0.0>\n",
            "<interview.n.01: PosScore=0.0 NegScore=0.0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_list = pos_tag(word_tokenize(senti_sentence))\n",
        "\n",
        "def pos_to_wn(pos):\n",
        "  if pos[0] == 'N':\n",
        "    return wn.NOUN\n",
        "  elif pos[0] == 'V':\n",
        "    return wn.VERB\n",
        "  elif pos[0] == 'J':\n",
        "    return wn.ADJ\n",
        "  elif pos[0] == 'R':\n",
        "    return wn.ADV\n",
        "  return\n",
        "\n",
        "i = 0\n",
        "for word in senti_list:\n",
        "  wn_pos = pos_to_wn(tagged_list[i][1])\n",
        "  i += 1\n",
        "  if wn_pos and len(list(swn.senti_synsets(word, wn_pos))) > 0:\n",
        "    print(list(swn.senti_synsets(word, wn_pos))[0], end = \" \")\n",
        "    print(\"ObjScore=\" + str(list(swn.senti_synsets(word, wn_pos))[0].obj_score()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bV0Dc1wl9bi",
        "outputId": "861616c1-72e6-4331-e898-6599886ca494"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<be.v.01: PosScore=0.25 NegScore=0.125> ObjScore=0.625\n",
            "<incredibly.r.01: PosScore=0.0 NegScore=0.5> ObjScore=0.5\n",
            "<stress.v.01: PosScore=0.0 NegScore=0.0> ObjScore=1.0\n",
            "<approaching.s.01: PosScore=0.0 NegScore=0.0> ObjScore=1.0\n",
            "<interview.n.01: PosScore=0.0 NegScore=0.0> ObjScore=1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentiWordNet can be used to find the emotional values of synsets according to the WordNet database. The stored values for each synset are positivity, negativity, and objectivity, which are straightforward. The first code block here uses a naive approach, assuming that the first synset found for each word has the correct definition. This surprisingly yielded elements for the first two words in the sentence, so I took another approach using POS tagging to try and find the correct definition for each word, ignoring stop words or words not in the WordNet database. The given scores usually make sense, though I am surprised that stress has a negative score of zero. It is possible that the wrong form of the word was chosen, but it is also possible that this word is not seen as necessarily negative for whatever reason. With these polarity scores, it could be possible to find the general mood or emotions behind a body of text by comparing the average or total positivity and negativity to a larger corpus, and the same thing can be done with objectivity scores to try and isolate bias."
      ],
      "metadata": {
        "id": "AAsj8O3opIup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collocation"
      ],
      "metadata": {
        "id": "hZlSOlVhqgvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text4.collocations())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6-18h1EqgSJ",
        "outputId": "bc4ec746-761d-47b4-ca65-332793672547"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(token.lower() for token in text4.tokens)\n",
        "unique_words = len(set(text4))\n",
        "prob_col = text.count(\"fellow citizens\") / unique_words\n",
        "prob_fellow = text.count(\"fellow\") / unique_words\n",
        "prob_citizens = text.count(\"citizens\") / unique_words\n",
        "\n",
        "print(\"PMI: \" + str(math.log2(prob_col / (prob_fellow * prob_citizens))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjv4XAOivhW3",
        "outputId": "e2984c65-1afd-4354-c7ac-8131ced5a4bc"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PMI: 4.132057790928088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A collocation is a series of words that carries more meaning than the sum of its parts, so much so that replacing a word in the collocation with a synonym would still change the meaning of the phrase. Collocations can be found by seeing if words appear more frequently together than they would by random chance, which can be measured with point-wise mutual information. PMI is the logarithm of the conditional probability of a set of words appearing together. For bigrams, this would be the logarithm of the probability of both words appearing together divided by the probabilities of each individual word in a given text. For the inaugural address, 'fellow citizens' has a PMI of about 4.13, which is quite high. From this, it can be assumed that 'fellow citizens' is a collocation. This was identified by the NLTK collocations function, so that seems to be accurate, though from this sample of code it is unknown if the list is exhaustive."
      ],
      "metadata": {
        "id": "aUzY96gsxVNg"
      }
    }
  ]
}